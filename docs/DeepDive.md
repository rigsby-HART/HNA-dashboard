# HART Tool Deep Dive

The first part will be less technical, should be able to interpreted by non-developers.  The second part will hopefully
give a good starting point to any developers hoping to make changes to this repo.

The main site, hart.ubc.ca is hosted separately, while the HART tool itself is hosted on Koyeb, accessed through an 
embed on the ubc page.  The entire stack is written in python, the frontend being generated by the python library Dash, 
by plotly. The backend data analysis logic is also in Python and mostly in Pandas, a python data analysis library.

## Data Flow 
![dataflow.png](dataflow.png)
1. The user accesses the page through a device capable of accessing the web, probably a computer.
2. The webpage responds to the request, the UBC HART page is specifically hosted through a third party company using
WordPress, after the main domain (ubc.ca) routes to our subdomain (hart prefix).  The WordPress server delivers all the 
static content, including an embed to the HART tools service, hosted on Koyeb
3. After Koyeb receives the request, it passes it to our specific instance (HART tool uses a medium size instance)
4. Our instance has a Gunicorn server running in front of our Dash app to help handle requests.  Dash is built on Flask,
and is not meant to handle all http calls, Gunicorn helps parse these calls for our app
5. Dash is the framework in which the components are organized
6. Plotly is used to generate all the graphs dynamically when required.  The actual math and table organization is done
with various python libraries including pandas and other data manipulation libraries.
7. The data that is used to return results is sourced from Canada census data, gone through processing to be stored as a
SQLite db file.  All data used for any graphs or results are sourced from this database.


## Repo structure

This project was initially developed with Licker Geo, but has been internally worked on since 2023-10.

Changelog:

10-19-2023 - I'm currently on refactoring the codebase for easier implementation of future features, and also easier 
readability.  For context, when I first stated, there were 5 total python files, 4 of which with 1-2k lines each

The data is obtained from Canadian censuses, and stored with their own tools.  This data is stored in high dimensional
methods (hundreds of different keys), and must be queried using their tool.  This data is then exported to csv, and
parsed using the `db_uploader.py` into a sqlite `hart2021.db` file.  This acts as our database for all later non geo
requests.

The production stack is served through either Dockerfile or Koyeb's Procfile, both are functional as of time of writing.
By running `gunicorn app:server`, you are able to host a local version of the server at port 8000.  This makes gunicorn
host the Flask app (Dash is just a framework on top of Flask).
The way the data flows through the individual python files are:

1. `app.py` is the root of all the pages.  It acts as a router to the specific each page.  `pages/page1.py` is the map,
while `page2.py` to `page4.py` are the data tables.
2. All of these pages are written with a Dash framework.  As such, they are able to share information about the current
selected geography using `dcc.store` variables (can be found in `helpers/table_helper.py`)
3. The table information is all sourced from `create_engine.py` to prevent redundant memory usage.  The file dynamically
imports the data from all years using `pandas` library, and does some simple data cleaning.
4. Each page serves a copy of a static page, with a default region hardcoded, then on page load, Dash performs an initial
call onto each callback function, which then processes the actual selected region from `page1.py`